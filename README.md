# Chat E-Book AI

**Chat E-Book AI** is a Retrieval-Augmented Generation (RAG) system designed to allow users to upload PDF documents (such as e-books) and ask questions about their content. The system processes the PDF, extracts and chunks the text, stores embeddings in a vector database, and uses a large language model (LLM) to generate accurate responses to user queries based on the document's content.

This project leverages modern NLP techniques, including text embedding, vector search, and language model generation, to provide a seamless question-answering experience for PDF documents.

---

## Table of Contents
- [Project Overview](#project-overview)
- [System Architecture](#system-architecture)
- [Components](#components)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

---

## Project Overview

**Chat E-Book AI** enables users to:
- Upload PDF documents (e.g., e-books, research papers).
- Ask questions about the content of the uploaded PDF.
- Receive accurate, contextually relevant answers generated by an LLM, augmented with retrieval from the document.

The system uses a Retrieval-Augmented Generation (RAG) approach, which combines:
- **Retrieval**: Searching for relevant sections of the document using vector similarity.
- **Generation**: Using a large language model (LLM) to generate a response based on the retrieved content and the user's query.

This ensures that the answers are grounded in the actual content of the document while leveraging the generative capabilities of the LLM.

---

## System Architecture

The following diagram illustrates the architecture of the RAG flow used in this project:

![RAG Flow Architecture](/rag-pipeline.png)  

The system follows a step-by-step process to handle PDF uploads, process user queries, and generate responses:

1. **PDF Upload**: The user uploads a PDF document.
2. **Text Extraction**: The text is extracted from the PDF using PyMuPDF or OCR.
3. **Chunking**: The extracted text is broken into smaller chunks (e.g., sentences or paragraphs).
4. **Sentence Embedding**: Each chunk is converted into a vector embedding using a pre-trained embedding model.
5. **Vector Database**: The embeddings are stored in a vector database (Elasticsearch) for efficient retrieval.
6. **User Query**: The user submits a question about the document.
7. **Query Embedding**: The user's question is embedded into a vector.
8. **Retrieval**: The system queries the vector database to find the top-k most relevant text chunks.
9. **Prompt Building**: A prompt is constructed using the user's question and the retrieved chunks.
10. **LLM Generation**: The prompt is sent to the LLM (Google's Gemini) to generate a response.
11. **Response**: The generated response is returned to the user.

---

## Components

The system consists of the following key components:

- **PDF Upload**: Handles the uploading of PDF documents.
- **Text Extraction**: Uses PyMuPDF or OCR to extract text from the PDF.
- **Chunking**: Splits the extracted text into smaller, manageable chunks.
- **Sentence Embedding**: Converts text chunks into vector embeddings using a pre-trained model.
- **Vector Database (Elasticsearch)**: Stores the embeddings for fast similarity search.
- **Query Embedding**: Embeds the user's question into a vector for retrieval.
- **Retrieval**: Retrieves the top-k most relevant chunks from the vector database based on the query embedding.
- **Prompt Building**: Combines the user's question and the retrieved chunks to form a prompt for the LLM.
- **LLM (Gemini)**: Generates a response based on the constructed prompt.
- **Response Generation**: Returns the LLM's answer to the user.

---

## How It Works

1. **Upload a PDF**: The user uploads a PDF document to the system.
2. **Process the PDF**: The system extracts text from the PDF, chunks it, and stores the embeddings in a vector database.
3. **Ask a Question**: The user submits a question related to the document.
4. **Retrieve Relevant Chunks**: The system embeds the question and retrieves the most relevant text chunks from the document.
5. **Generate a Response**: The retrieved chunks and the question are used to build a prompt, which is sent to the LLM to generate a response.
6. **Receive the Answer**: The system returns the generated answer to the user.

---

## How It Works

1. **Upload a PDF**: The user uploads a PDF document to the system.
2. **Process the PDF**: The system extracts text from the PDF, chunks it, and stores the embeddings in a vector database.
3. **Ask a Question**: The user submits a question related to the document.
4. **Retrieve Relevant Chunks**: The system embeds the question and retrieves the most relevant text chunks from the document.
5. **Generate a Response**: The retrieved chunks and the question are used to build a prompt, which is sent to the LLM to generate a response.
6. **Receive the Answer**: The system returns the generated answer to the user.

This process ensures that the responses are accurate and contextually relevant to both the user's query and the document's content.

---

## Installation

To set up the project locally, follow these steps:

1. **Clone the repository**:
   ```bash
   git clone https://github.com/phdhuy/chat-ebook-ai.git
   cd chat-ebook-ai
   ```

2. **Install dependencies**:
   Ensure you have Python 3.8+ installed. Then, install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up Elasticsearch**:
   - Install and run Elasticsearch locally or use a cloud instance.
   - Configure the connection details in the project's configuration file.

4. **Set up API keys**:
   - Obtain an API key for the LLM (Gemini) and add it to the configuration.

5. **Run the application**:
   ```bash
   python main.py
   ```

---

## Usage

1. **Upload a PDF**:
   - Use the web interface or API to upload a PDF document.

2. **Ask Questions**:
   - Once the PDF is processed, you can ask questions about its content.
   - The system will retrieve relevant sections and generate an answer using the LLM.

3. **View Responses**:
   - The generated response will be displayed or returned via the API.

---

## Contributing

Contributions are welcome! To contribute:

1. Fork the repository.
2. Create a new branch for your feature or bugfix.
3. Submit a pull request with a detailed description of your changes.

Please ensure that your code follows the project's coding standards and includes appropriate tests.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---
